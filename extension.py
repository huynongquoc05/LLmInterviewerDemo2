from typing import Optional, List

from pymongo import MongoClient

from extensions import embedding_manager


def summarize_knowledge_with_llm(knowledge_text: str, topic: str, outline: list[str], llm):
    """
    D√πng LLM ƒë·ªÉ t√≥m t·∫Øt v√† ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ngu·ªìn t√†i li·ªáu RAG.
    """
    if not knowledge_text or len(knowledge_text.strip()) == 0:
        return {"summary": "(Kh√¥ng c√≥ t√†i li·ªáu)", "quality_report": "Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ ƒë√°nh gi√°."}

    outline_str = "\n".join(f"- {item}" for item in outline or [])

    prompt = f"""
    B·∫°n l√† chuy√™n gia trong lƒ©nh v·ª±c li√™n quan ƒë·∫øn {topic}, h√£y gi√∫p ƒë√°nh gi√° v√† t√≥m t·∫Øt t√†i li·ªáu ph·ªèng v·∫•n sau.
    
    CH·ª¶ ƒê·ªÄ: {topic}
    OUTLINE (m·ª•c ti√™u ki·∫øn th·ª©c): 
    {outline_str}

    --- T√ÄI LI·ªÜU TRUY V·∫§N ---
    {knowledge_text}  # Gi·ªõi h·∫°n ƒë·ªÉ tr√°nh token overflow
    ƒê√¢y l√† ngu·ªìn t√†i li·ªáu truy v·∫•n t·ª´ k·ªπ thu·∫≠t RAG d·ª±a tr√™n t·ª´ng  item in outline, m·ªói item l√† 1 query
    H√ÉY TR·∫¢ V·ªÄ B·∫£n t√≥m t·∫Øt 
    """

    result = llm.invoke(prompt)
    return result


import os
import pandas as pd
from datetime import datetime
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings


def build_cv_vectorstore_from_candidates(candidates, embedding_model=None, base_dir="vectorstores/cv"):
    """
    T·∫°o vectorstore FAISS cho danh s√°ch th√≠ sinh.
    """
    os.makedirs(base_dir, exist_ok=True)

    # 1Ô∏è‚É£ Load data
    if isinstance(candidates, str):
        df = pd.read_csv(candidates)
    else:
        df = pd.DataFrame(candidates)
    print(df.head(10))
    def row_to_text(row):
        return ", ".join(f"{col}: {val}" for col, val in row.items())

    texts = [row_to_text(r) for _, r in df.iterrows()]

    # 3Ô∏è‚É£ N·∫øu kh√¥ng truy·ªÅn model th√¨ t·∫°o m·ªõi
    if embedding_model is None:
        embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large-instruct")

    # 4Ô∏è‚É£ T·∫°o v√† l∆∞u vectorstore
    vectorstore = FAISS.from_texts(texts, embedding_model)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_path = os.path.join(base_dir, f"cv_{timestamp}")
    vectorstore.save_local(save_path)

    print(f"‚úÖ CV Vectorstore saved to {save_path}")
    return save_path





import requests
import os
from GetApikey import get_api_key_elevenlab


# extension.py
from GetApikey import get_api_key_elevenlab
import requests

import requests
import json  # Import th√™m th∆∞ vi·ªán n√†y ƒë·ªÉ in JSON ƒë·∫πp h∆°n (n·∫øu mu·ªën)


def generate_voice_ElevenLab(text, output_path="output.mp3"):
    API_KEY = get_api_key_elevenlab()

    # Debug: Ki·ªÉm tra xem API Key c√≥ l·∫•y ƒë∆∞·ª£c kh√¥ng (ch·ªâ in 4 k√Ω t·ª± ƒë·∫ßu ƒë·ªÉ b·∫£o m·∫≠t)
    if API_KEY:
        print(f"üîë API Key loaded: {API_KEY[:4]}...****")
    else:
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y API Key.")
        return None

    url = "https://api.elevenlabs.io/v1/text-to-speech/pNInz6obpgDQGcFmaJgB"
    headers = {
        "xi-api-key": API_KEY,
        "Content-Type": "application/json"
    }

    # L∆∞u √Ω: ElevenLabs ƒë√¥i khi y√™u c·∫ßu tham s·ªë 'voice_speed' n·∫±m trong model setting ho·∫∑c t√πy model.
    # N·∫øu d√πng turbo v2.5, c·∫•u tr√∫c n√†y c∆° b·∫£n l√† ·ªïn.
    data = {
        "text": text,
        "model_id": "eleven_turbo_v2_5",
        "voice_settings": {
            "stability": 0.5,  # Gi·∫£m xu·ªëng m·ª©c trung b√¨nh ƒë·ªÉ test
            "similarity_boost": 0.8
        }
        # "voice_speed": 1.4 # C·∫©n th·∫≠n tham s·ªë n√†y, n·∫øu API kh√¥ng h·ªó tr·ª£ n√≥ ·ªü root level s·∫Ω b√°o l·ªói 400 ho·∫∑c 422.
        # Nh∆∞ng ·ªü ƒë√¢y b·∫°n b·ªã 401 n√™n l√† do Key.
    }

    try:
        res = requests.post(url, json=data, headers=headers)

        if res.status_code == 200:
            with open(output_path, "wb") as f:
                f.write(res.content)
            return output_path

        elif res.status_code == 429:
            print("‚ö†Ô∏è H·∫øt limit ElevenLabs (429).")
            return None

        else:
            # --- ƒê√ÇY L√Ä PH·∫¶N QUAN TR·ªåNG ƒê·ªÇ DEBUG ---
            print(f"‚ö†Ô∏è L·ªói ElevenLabs: {res.status_code}")
            try:
                # C·ªë g·∫Øng in l·ªói d·∫°ng JSON cho d·ªÖ ƒë·ªçc
                error_detail = res.json()
                print("üìÑ Chi ti·∫øt l·ªói:", json.dumps(error_detail, indent=2))
            except:
                # N·∫øu kh√¥ng ph·∫£i JSON th√¨ in raw text
                print("üìÑ Chi ti·∫øt l·ªói (Raw):", res.text)
            return None

    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói g·ª≠i request ElevenLabs: {e}")
        return None

import requests
import os

def generate_voice_LocalTTS(
    text,
    output_path="output.mp3",
    voice="vi-VN-NamMinhNeural",
    speed=1.0,
    model="tts-1"
):
    """
    Sinh gi·ªçng n√≥i t·ª´ text b·∫±ng container TTS local (ch·∫°y ·ªü localhost:5050).
    Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n file n·∫øu th√†nh c√¥ng, None n·∫øu l·ªói.
    """
    try:
        # API endpoint & headers
        url = "http://localhost:5051/v1/audio/speech"
        api_key = "your_api_key_here"  # Cho ph√©p l·∫•y t·ª´ env
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        # Request body
        data = {
            "model": model,
            "input": text,
            "voice": voice,
            "response_format": "mp3",
            "speed": speed
        }

        # G·ª≠i request
        res = requests.post(url, headers=headers, json=data)

        # X·ª≠ l√Ω ph·∫£n h·ªìi
        if res.status_code == 200:
            with open(output_path, "wb") as f:
                f.write(res.content)
            return output_path
        else:
            print(f"‚ö†Ô∏è L·ªói LocalTTS: {res.status_code} - {res.text}")
            return None

    except Exception as e:
        print(f"‚ùå L·ªói khi g·ªçi LocalTTS: {e}")
        return None

import struct
from google import genai
from google.genai import types
from GetApikey import loadapi
from pydub import AudioSegment
import os

def generate_voice_Gemini_simple(text, output_path="gemini.mp3", voice="alnilam"):
    """
    Sinh voice t·ª´ Google Gemini b·∫±ng API non-stream.
    Convert audio raw L16 ‚Üí WAV ‚Üí MP3.
    """

    api_key = loadapi()
    if not api_key:
        print("‚ùå Kh√¥ng t√¨m th·∫•y API Key Gemini.")
        return None

    try:
        client = genai.Client(api_key=api_key)
        model = "gemini-2.5-flash-preview-tts"

        res = client.models.generate_content(
            model=model,
            contents=text,
            config=types.GenerateContentConfig(
                response_modalities=["audio"],  # ch·ªâ c·∫ßn audio
                speech_config=types.SpeechConfig(
                    voice_config=types.VoiceConfig(
                        prebuilt_voice_config=types.PrebuiltVoiceConfig(
                            voice_name=voice
                        )
                    )
                ),
            ),
        )

        # L·∫•y bytes raw PCM
        audio_bytes = res.candidates[0].content.parts[0].inline_data.data

        # --- t·∫°o WAV header ---
        sample_rate = 24000  # Gemini L16 chu·∫©n 24kHz
        bits_per_sample = 16
        num_channels = 1
        byte_rate = sample_rate * num_channels * bits_per_sample // 8
        block_align = num_channels * bits_per_sample // 8
        subchunk2_size = len(audio_bytes)
        chunk_size = 36 + subchunk2_size

        wav_header = struct.pack(
            "<4sI4s4sIHHIIHH4sI",
            b"RIFF",
            chunk_size,
            b"WAVE",
            b"fmt ",
            16,
            1,
            num_channels,
            sample_rate,
            byte_rate,
            block_align,
            bits_per_sample,
            b"data",
            subchunk2_size,
        )

        wav_path = output_path.replace(".mp3", ".wav")
        with open(wav_path, "wb") as f:
            f.write(wav_header)
            f.write(audio_bytes)

        # Convert WAV ‚Üí MP3
        audio = AudioSegment.from_file(wav_path, format="wav")
        audio.export(output_path, format="mp3")
        os.remove(wav_path)

        return output_path

    except Exception as e:
        print(f"‚ùå L·ªói Gemini TTS: {e}")
        return None
import struct
import os
from pydub import AudioSegment
from google import genai
from google.genai import types


# Gi·∫£ s·ª≠ loadapi ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü ngo√†i
# from your_module import loadapi

# def generate_voice_Gemini_simple(text, output_path="gemini.mp3", voice="alnilam", speed=1.4):
#     """
#     Sinh voice t·ª´ Google Gemini.
#     - speed: T·ªëc ƒë·ªô ƒë·ªçc (m·∫∑c ƒë·ªãnh 1.0).
#       > 1.0 l√† nhanh h∆°n, ch·ªâ h·ªó tr·ª£ t·ªët nh·∫•t vi·ªác TƒÇNG t·ªëc ƒë·ªô b·∫±ng pydub.
#     """
#
#     api_key = loadapi()
#     print(api_key)
#     if not api_key:
#         print("‚ùå Kh√¥ng t√¨m th·∫•y API Key Gemini.")
#         print(api_key)
#         return None
#
#     try:
#         client = genai.Client(api_key=api_key)
#         # L∆∞u √Ω: model n√†y c√≥ th·ªÉ thay ƒë·ªïi t√™n t√πy th·ªùi ƒëi·ªÉm Google c·∫≠p nh·∫≠t
#         model = "gemini-2.5-flash-preview-tts"
#
#         res = client.models.generate_content(
#             model=model,
#             contents=text,
#             config=types.GenerateContentConfig(
#                 response_modalities=["audio"],
#                 speech_config=types.SpeechConfig(
#                     voice_config=types.VoiceConfig(
#                         prebuilt_voice_config=types.PrebuiltVoiceConfig(
#                             voice_name=voice
#                         )
#                     )
#                 ),
#             ),
#         )
#
#         # L·∫•y bytes raw PCM
#         # L∆∞u √Ω: C·∫•u tr√∫c response c√≥ th·ªÉ kh√°c nhau t√πy phi√™n b·∫£n SDK, code c≈© c·ªßa b·∫°n:
#         # audio_bytes = res.candidates[0].content.parts[0].inline_data.data
#         # N·∫øu code tr√™n l·ªói, h√£y th·ª≠ debug res ƒë·ªÉ xem c·∫•u tr√∫c ƒë√∫ng.
#         # D∆∞·ªõi ƒë√¢y l√† c√°ch l·∫•y an to√†n th∆∞·ªùng th·∫•y:
#         audio_bytes = None
#         for part in res.candidates[0].content.parts:
#             if part.inline_data:
#                 audio_bytes = part.inline_data.data
#                 break
#
#         if not audio_bytes:
#             print("‚ùå Gemini kh√¥ng tr·∫£ v·ªÅ d·ªØ li·ªáu audio.")
#             return None
#
#         # --- T·∫°o WAV header ---
#         sample_rate = 24000
#         bits_per_sample = 16
#         num_channels = 1
#         subchunk2_size = len(audio_bytes)
#         byte_rate = sample_rate * num_channels * bits_per_sample // 8
#         block_align = num_channels * bits_per_sample // 8
#         chunk_size = 36 + subchunk2_size
#
#         wav_header = struct.pack(
#             "<4sI4s4sIHHIIHH4sI",
#             b"RIFF",
#             chunk_size,
#             b"WAVE",
#             b"fmt ",
#             16,
#             1,
#             num_channels,
#             sample_rate,
#             byte_rate,
#             block_align,
#             bits_per_sample,
#             b"data",
#             subchunk2_size,
#         )
#
#         wav_path = output_path.replace(".mp3", ".wav")
#         with open(wav_path, "wb") as f:
#             f.write(wav_header)
#             f.write(audio_bytes)
#
#         # --- X·ª¨ L√ù T·ªêC ƒê·ªò B·∫∞NG PYDUB ---
#         audio = AudioSegment.from_file(wav_path, format="wav")
#
#         if speed != 1.0:
#             if speed > 1.0:
#                 # H√†m speedup gi√∫p tƒÉng t·ªëc m√† KH√îNG ƒë·ªïi cao ƒë·ªô (pitch)
#                 # chunk_size v√† crossfade gi√∫p √¢m thanh ƒë·ª° b·ªã m√©o/ng·∫Øt qu√£ng
#                 audio = audio.speedup(playback_speed=speed, chunk_size=150, crossfade=25)
#             else:
#                 # Pydub m·∫∑c ƒë·ªãnh kh√¥ng c√≥ h√†m slow_down t·ªët (s·∫Ω b·ªã ƒë·ªïi gi·ªçng tr·∫ßm xu·ªëng).
#                 # N·∫øu mu·ªën ch·∫≠m l·∫°i ƒë∆°n gi·∫£n (ch·∫•p nh·∫≠n gi·ªçng tr·∫ßm):
#                 # new_sample_rate = int(audio.frame_rate * speed)
#                 # audio = audio._spawn(audio.raw_data, overrides={'frame_rate': new_sample_rate})
#                 # audio = audio.set_frame_rate(24000)
#                 print("‚ö†Ô∏è L∆∞u √Ω: Pydub ch·ªâ h·ªó tr·ª£ t·ªët vi·ªác tƒÉng t·ªëc ƒë·ªô (>1.0).")
#
#         audio.export(output_path, format="mp3")
#
#         # D·ªçn d·∫πp file wav t·∫°m
#         if os.path.exists(wav_path):
#             os.remove(wav_path)
#
#         return output_path
#
#     except Exception as e:
#         print(f"‚ùå L·ªói Gemini TTS: {e}")
#         return None

def get_vectorstore_chunks(vectorstore_id, mongo_uri="mongodb://localhost:27017/"):
    """
    Tr√≠ch xu·∫•t n·ªôi dung t·ª´ng chunk trong vectorstore
    - ƒê·ªçc metadata vectorstore t·ª´ MongoDB
    - T·ª± ƒë·ªông ch·ªçn ƒë√∫ng model embedding
    - Tr·∫£ v·ªÅ danh s√°ch c√°c ƒëo·∫°n vƒÉn b·∫£n (chunks)
    """
    from bson import ObjectId

    # K·∫øt n·ªëi DB
    client = MongoClient(mongo_uri)
    db = client["interviewer_ai"]
    vs = db["vectorstores"].find_one({"_id": ObjectId(vectorstore_id)})

    if not vs:
        raise ValueError(f"Vectorstore {vectorstore_id} not found")

    # L·∫•y th√¥ng tin t·ª´ metadata
    path = vs.get("vectorstore_path")
    if not path:
        raise ValueError(f"Vectorstore {vectorstore_id} missing vectorstore_path")

    # L·∫•y model embedding ch√≠nh x√°c t·ª´ metadata
    model_name = vs.get("model_name") or vs.get("model_info", {}).get("name")
    if not model_name:
        model_name = "intfloat/multilingual-e5-large-instruct"  # fallback an to√†n

    # Kh·ªüi t·∫°o embedding v√† load FAISS
    embeddings = embedding_manager.get_model(model_name=model_name, device="cpu")
    vs_local = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)

    # Tr√≠ch xu·∫•t n·ªôi dung c√°c chunk
    docs = []
    for i, doc in enumerate(vs_local.docstore._dict.values()):
        docs.append({
            "index": i + 1,
            "content": doc.page_content,
            "metadata": doc.metadata or {},
        })

    return docs


class KnowledgeBuilder:
    """X√¢y d·ª±ng knowledge context c√≥ m·ªü r·ªông ng·ªØ c·∫£nh v√† ƒë·ªãnh d·∫°ng d·ªÖ ƒë·ªçc h∆°n."""

    def __init__(self, knowledge_db: Optional[FAISS] = None, fetch_surrounding: bool = True, window: int = 1):
        self._knowledge_db = knowledge_db
        self.retriever = None
        self.fetch_surrounding = fetch_surrounding
        self.window = window
        if knowledge_db:
            self.retriever = knowledge_db.as_retriever(search_kwargs={"k": 5})

    @property
    def knowledge_db(self):
        return self._knowledge_db

    @knowledge_db.setter
    def knowledge_db(self, value):
        self._knowledge_db = value
        if value:
            self.retriever = value.as_retriever(search_kwargs={"k": 5})
            print("üîÑ retriever auto-updated from new knowledge_db")
        else:
            self.retriever = None
            print("‚ö†Ô∏è retriever cleared (knowledge_db=None)")

    def _fetch_surrounding_chunks(self, doc):
        """L·∫•y th√™m c√°c chunk li·ªÅn k·ªÅ (tr∆∞·ªõc/sau) n·∫øu c√≥."""
        if not self.fetch_surrounding or "chunk_index" not in doc.metadata:
            return [doc]

        index = doc.metadata["chunk_index"]
        source = doc.metadata.get("source")
        all_docs = self.knowledge_db.docstore._dict.values()

        context_docs = [doc]
        for i in range(1, self.window + 1):
            prev_doc = next(
                (d for d in all_docs if d.metadata.get("source") == source and d.metadata.get("chunk_index") == index - i),
                None)
            next_doc = next(
                (d for d in all_docs if d.metadata.get("source") == source and d.metadata.get("chunk_index") == index + i),
                None)
            if prev_doc:
                context_docs.insert(0, prev_doc)
            if next_doc:
                context_docs.append(next_doc)
        return context_docs

    def build_context(self, topic: str, outline: Optional[List[str]] = None) -> str:
        """T·∫°o knowledge context v·ªõi ƒë·ªãnh d·∫°ng gi√∫p LLM d·ªÖ ƒë·ªçc h∆°n."""
        results = []

        queries = outline if outline else [topic]
        for item in queries:
            query = f"{topic} {item}" if outline else item
            docs = self.retriever.invoke(query)
            for doc in docs:
                results.extend(self._fetch_surrounding_chunks(doc))

        # Lo·∫°i tr√πng l·∫∑p
        seen = set()
        unique_docs = []
        for doc in results:
            if doc.page_content not in seen:
                seen.add(doc.page_content)
                unique_docs.append(doc)

        # üîπ ƒê·ªãnh d·∫°ng context r√µ r√†ng h∆°n
        formatted_sections = []
        for idx, doc in enumerate(unique_docs, 1):
            source = doc.metadata.get("source", "Kh√¥ng r√µ ngu·ªìn")
            section = (
                f"### M·ª•c {idx}\n"
                f"**Ngu·ªìn:** {source}\n"
                f"**N·ªôi dung:**\n{doc.page_content.strip()}\n"
            )
            formatted_sections.append(section)

        knowledge_context = "\n\n---\n\n".join(formatted_sections)
        return knowledge_context

